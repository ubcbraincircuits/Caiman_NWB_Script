{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The DMCBH Open Science Initiative\n",
    "\n",
    "The goal of our Open Science Initiative is to establish local support for, and a framework leading to, the transformation of the UBC Djavad Mowafaghian Centre for Brain Health into an Open Science Institute modelled after the pioneering example of the McGill Neurological Institute (MNI). Developing and adapting open science principles to suit the unique needs of our Centre necessitates collecting information on attitudes, behaviours and perceived norms, as they pertain to open science, from everyone in our research community - faculty, staff and students.  In complement to the focus groups and interviews we have been conducting with you, we have provided a script below to bridge the gap between conducting analysis in Caiman (an open-source tool for scalable calcium imaging data analysis) and working with an NWB file format. NWB (Neurodata Without Borders) is a data standard which enables neuroscientists to share and use analysis tools for neurophysiology data in hopes of breaking down barriers in data sharing.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://pbs.twimg.com/profile_images/1133961932950532096/15M5Fvdy_400x400.png\" width=\"200\" height=\"200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html><head><meta content=\"text/html; charset=UTF-8\" http-equiv=\"content-type\"><style type=\"text/css\">ol</style></head><body class=\"c5\"><p class=\"c0 c4\"><span class=\"c3\"></span></p><p class=\"c2 title\" id=\"h.rrbabt268i6e\"><h1>CaImAn&rsquo;s Demo Pipeline</h1></p><p class=\"c0\"><span class=\"c3\">\n",
    "This script follows closely the demo_pipeline.py script but uses the\n",
    "Neurodata Without Borders (NWB) file format for loading the input and saving\n",
    "the output. It is meant as an example on how to use NWB files with CaImAn.\n",
    "authors: @agiovann and @epnev. The notebook has been adjusted a bit and has more descriptions on the steps of the analysis in order to better suit the needs of the centre.\n",
    "\n",
    "    \n",
    "Additional note: make sure caiman is activated prior to launching jupyter notebook to run this script. This can be done so by navigating to the caiman_data folder with the terminal and using <br> <br> `source activate caiman` <br> <br>Once you have done so, you can execute the code segments that follow in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bpl\n",
    "import cv2\n",
    "import glob\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except():\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        # this is used for debugging purposes only. allows to reload classes\n",
    "        # when changed\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "import caiman as cm\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "from caiman.source_extraction.cnmf import cnmf as cnmf\n",
    "from caiman.source_extraction.cnmf import params as params\n",
    "from caiman.utils.utils import download_demo\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour\n",
    "from caiman.paths import caiman_datadir\n",
    "bpl.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up logger (optional)\n",
    "You can log to a file using the filename parameter, or make the output more or less verbose by setting level to `logging.DEBUG`, `logging.INFO`, `logging.WARNING`, or `logging.ERROR`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\n",
    "                    \"%(relativeCreated)12d [%(filename)s:%(funcName)20s():%(lineno)s]\"\\\n",
    "                    \"[%(process)d] %(message)s\",\n",
    "                    level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select file(s) to be processed or download them if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [os.path.join(caiman_datadir(), 'example_movies/cropped_testv2.nwb')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates save path can be same or different from raw data path. Please note that if you plan on using the Caiman GUI for further analysis once the nwb file is created, you need to load in the estimates nwb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(caiman_datadir(), 'example_movies/caiman_estimates_cropped_tifv2.nwb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now set dataset dependent parameters below. fr is the imaging rate in frames per second and decay_time is the length of a typical transient in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = 15.\n",
    "decay_time = 0.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the file and save it in the NWB format (if it doesn't exist already). The download_demo function will download the tif file indicated and return the path to the file which will be stored in the caiman_data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fnames[0]):\n",
    "    fnames_orig = 'm2_00002_cropped.tif'  # filename to be processed\n",
    "    if fnames_orig in ['m2_00002_cropped.tif', 'demoMovie.tif']:\n",
    "        fnames_orig = [download_demo(fnames_orig)]\n",
    "    orig_movie = cm.load(fnames_orig, fr=fr) #loads in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file can be now saved in an NWB format with additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_movie.save(fnames[0], sess_desc='test', identifier='demo 1',\n",
    "        imaging_plane_description='single plane',\n",
    "        emission_lambda=520.0, indicator='GCAMP6f',\n",
    "        location='parietal cortex',\n",
    "        experimenter='Marja Sepers', lab_name='Raymond Lab',\n",
    "        institution='UBC',\n",
    "        experiment_description='Experiment Description',\n",
    "        session_id='Session 1',\n",
    "        var_name_hdf5='TwoPhotonSeries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set up parameters for data and motion correction. Upon doing so, we pass all the params set as a dictionary. The goal of motion correction is to enable mapping the pixels to the same spatial positions.\n",
    "\n",
    "Please note that there is a tradeoff between how non-rigid motion is vs the size of the patches.\n",
    "Making the transformation peacewise rigid is more precise but will take a longer time for the analysis to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxy = (2., 2.)  # spatial resolution in x and y in (um per pixel)\n",
    "# note the lower than usual spatial resolution here\n",
    "max_shift_um = (12., 12.)  # maximum shift in um\n",
    "patch_motion_um = (100., 100.)  # patch size for non-rigid correction in um\n",
    "pw_rigid = True       # flag to select rigid vs pw_rigid motion correction\n",
    "max_shifts = [int(a/b) for a, b in zip(max_shift_um, dxy)] # maximum allowed rigid shift in pixels\n",
    "strides = tuple([int(a/b) for a, b in zip(patch_motion_um, dxy)]) # start a new patch for pw-rigid motion correction every x pixels\n",
    "overlaps = (24, 24) # overlap between patches (size of patch in pixels: strides+overlaps)\n",
    "max_deviation_rigid = 3 # maximum deviation allowed for patch with respect to rigid shifts\n",
    "\n",
    "\n",
    "mc_dict = {\n",
    "    'fnames': fnames,\n",
    "    'fr': fr,\n",
    "    'decay_time': decay_time,\n",
    "    'dxy': dxy,\n",
    "    'pw_rigid': pw_rigid,\n",
    "    'max_shifts': max_shifts,\n",
    "    'strides': strides,\n",
    "    'overlaps': overlaps,\n",
    "    'max_deviation_rigid': max_deviation_rigid,\n",
    "    'border_nan': 'copy',\n",
    "    #'var_name_hdf5': 'acquisition/TwoPhotonSeries'\n",
    "    'var_name_hdf5': 'TwoPhotonSeries'\n",
    "}\n",
    "\n",
    "opts = params.CNMFParams(params_dict=mc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the movie (optional)\n",
    "Play the movie (optional). This will require loading the movie in memory which in general is not needed by the pipeline. Displaying the movie uses the OpenCV library. Press `q` to close the video panel.\n",
    "\n",
    "Please note to set display_images to True if you wish to play the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images = True\n",
    "if display_images:\n",
    "    m_orig = cm.load_movie_chain(fnames, var_name_hdf5=opts.data['var_name_hdf5'])\n",
    "    ds_ratio = 0.2\n",
    "    moviehandle = m_orig.resize(1, 1, ds_ratio)\n",
    "    moviehandle.play(q_max=99.5, fr=60, magnification=2, do_loop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to start a cluster for parallel processing (this enables parallel motion correction by dividing the movie into chunks). This saves space and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='local', n_processes=None, single_thread=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to perform motion correction; the following line of code creates a motion correction object with the specified parameters. Note that the file is not loaded in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = MotionCorrect(fnames, dview=dview, var_name_hdf5=opts.data['var_name_hdf5'], **opts.get_group('motion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step runs a piecewise rigid motion correction using NoRMCorre. Note that the save_movie flag saves the motion corrected movie as a memory mapped file. \n",
    "\n",
    "A memory mapped file is an infrastructure which enables performing operations directly on the data stored on the hard drive. The default memory mapped file is created in order 'F' which is efficient for reading the file frame by frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.motion_correct(save_movie=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code enables you to compare the motion-corrected movie with the original. You can press q to exit. The left movie is the original and the right is the motion-corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_images:\n",
    "    m_orig = cm.load_movie_chain(fnames, var_name_hdf5=opts.data['var_name_hdf5'])\n",
    "    m_els = cm.load(mc.mmap_file)\n",
    "    ds_ratio = 0.2\n",
    "    moviehandle = cm.concatenate([m_orig.resize(1, 1, ds_ratio) - mc.min_mov*mc.nonneg_movie,\n",
    "                                    m_els.resize(1, 1, ds_ratio)], axis=2)\n",
    "    moviehandle.play(fr=60, q_max=99.5, magnification=2, do_loop=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_to_0 = 0 if mc.border_nan == 'copy' else mc.border_to_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the boundaries of the FOV if you used the copy option during motion correction (which is done so in the line of code above) but should be careful about the components near the boundaries. You can now memory map the file in order 'C'. This is efficient for reading the file pixel-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_new = cm.save_memmap(mc.mmap_file, base_name='memmap_', order='C',\n",
    "                            border_to_0=border_to_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now load the file and easily read a subset of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yr, dims, T = cm.load_memmap(fname_new)\n",
    "images = np.reshape(Yr.T, [T] + list(dims), order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now restart the cluster to clean up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.stop_server(dview=dview)\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='local', n_processes=None, single_thread=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Extraction and Deconvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source extraction and deconvolution is done based off a batch analysis pipeline. The field of view will be divided in patches which will be sent to processors as tensors. The processors solve each patch (by defining the neurons) and put them together again. In other words, the chunks are processed and put together by merging the borders. The goal of source extraction  is to find the spatial footprint and signal of neurons. The goal of deconvolution is to find the noise to version of the signal with the auto-regressive model.\n",
    "\n",
    "First, we set the required params for source extraction and deconvolution which will later be refined based on quality criteria.\n",
    "\n",
    "Additional info on the params set:\n",
    "<ul>\n",
    "<li>p: defines how deconvolution is performed (p=1 is appropiate for fast indicators with slow frame rates such as 30 Hz with GCamp6F; p=2 is appropiate for slow indicators with fast frame rates such as GCamp6s)</li>\n",
    "<li>gnb: neuropil (usually set to 1 but can also be set to 2 or 3 if the neuropil is complex)</li>   \n",
    "<li>merge_thresh: can even go up to 0.9 to avoid putting together different neurons</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1                    # order of the autoregressive system \n",
    "gnb = 1                  # number of global background components \n",
    "merge_thr = 0.85         # merging threshold, max correlation allowed\n",
    "rf = 15\n",
    "# half-size of the patches in pixels. e.g., if rf=25, patches are 50x50\n",
    "stride_cnmf = 6          # amount of overlap between the patches in pixels\n",
    "K = 4                    # number of components per patch (expected); usually overestimated\n",
    "gSig = [4, 4]            # expected half size of neurons in pixels\n",
    "# initialization method (if analyzing dendritic data using 'sparse_nmf')\n",
    "method_init = 'greedy_roi'\n",
    "ssub = 2                     # spatial subsampling during initialization\n",
    "tsub = 2                     # temporal subsampling during intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for component evaluation\n",
    "opts_dict = {'fnames': fnames,\n",
    "                'fr': fr,\n",
    "                'nb': gnb,\n",
    "                'rf': rf,\n",
    "                'K': K,\n",
    "                'gSig': gSig,\n",
    "                'stride': stride_cnmf,\n",
    "                'method_init': method_init,\n",
    "                'rolling_sum': True,\n",
    "                'merge_thr': merge_thr,\n",
    "                'n_processes': n_processes,\n",
    "                'only_init': True,\n",
    "                'ssub': ssub,\n",
    "                'tsub': tsub}\n",
    "\n",
    "opts.change_params(params_dict=opts_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run CNMF on patches by first extracting spatial and temporal components on patches and combining them. Deconvolution is turned off for this step by setting p to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.change_params({'p': 0})\n",
    "cnm = cnmf.CNMF(n_processes, params=opts, dview=dview)\n",
    "cnm = cnm.fit(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now plot the contours of found components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cn = cm.local_correlations(images, swap_dim=False)\n",
    "Cn[np.isnan(Cn)] = 0\n",
    "cnm.estimates.plot_contours(img=Cn)\n",
    "plt.title('Contour plots of found components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be saved in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm.estimates.Cn = Cn\n",
    "cnm.save(fname_new[:-4]+'hdf5')\n",
    "cm.movie(Cn).save(fname_new[:-5]+'_Cn.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refine and perform deconvolution based on quality criteria, we rerun the seeded CNMF on accepted patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm.params.change_params({'p': p})\n",
    "cnm2 = cnm.refit(images, dview=dview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality criteria used to rerun seeded CNMF are defined below:\n",
    "<ul>\n",
    "<li>min_SNR: how big the largest transient should be</li>\n",
    "<li>rval_thr: how similar spatial component is with extraction vs FOV</li>\n",
    "<li>cnn_thr: based on convolution network (supervised learning to differentiate neuron vs non-neuron)</li>\n",
    "</ul>\n",
    "\n",
    "We can now evaluate components in three ways:\n",
    "1) The shape of each component must be correlated with the data <br>\n",
    "2) A minimum peak SNR is required over the length of a transient <br>\n",
    "3) Each shape passes a CNN based classifier <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_SNR = 2  # signal to noise ratio for accepting a component\n",
    "rval_thr = 0.85  # space correlation threshold for accepting a component\n",
    "cnn_thr = 0.99  # threshold for CNN based classifier\n",
    "cnn_lowest = 0.1 # neurons with cnn probability lower than this value are rejected\n",
    "\n",
    "cnm2.params.set('quality', {'decay_time': decay_time,\n",
    "                            'min_SNR': min_SNR,\n",
    "                            'rval_thr': rval_thr,\n",
    "                            'use_cnn': True,\n",
    "                            'min_cnn_thr': cnn_thr,\n",
    "                            'cnn_lowest': cnn_lowest});\n",
    "cnm2.estimates.evaluate_components(images, cnm2.params, dview=dview)\n",
    "cnm2.estimates.Cn = Cn\n",
    "cnm2.save(fname_new[:-4] + 'hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now plot components after refinement. The left plot is that of the components with good spatial footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm2.estimates.plot_contours(img=Cn, idx=cnm2.estimates.idx_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code segment below enables you to view accepted and rejected traces. Please note that the first line enables who to view accepted components and the second line is for rejected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_images:\n",
    "    cnm2.estimates.view_components(images, img=Cn,\n",
    "                                    idx=cnm2.estimates.idx_components)\n",
    "    cnm2.estimates.view_components(images, img=Cn,\n",
    "                                    idx=cnm2.estimates.idx_components_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can update the the object with selected components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm2.estimates.select_components(use_object=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract dF/F values and show final traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm2.estimates.detrend_df_f(quantileMin=8, frames_window=250)\n",
    "cnm2.estimates.view_components(img=Cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also now reconstruct denoised movie. The movie on the left is the raw movie; the one in the middle is the reconstructed movie; the right one is the residual movie. There should be clear spatial footprint in the middle and almost no components in the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_images:\n",
    "    cnm2.estimates.play_movie(images, q_max=99.9, gain_res=2,\n",
    "                                magnification=2,\n",
    "                                bpx=border_to_0,\n",
    "                                include_bck=False) #enables you to include background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can stop the cluster and clean up log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.stop_server(dview=dview)\n",
    "log_files = glob.glob('*_LOG_*')\n",
    "for log_file in log_files:\n",
    "    os.remove(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, save the files in the original NWB file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm2.estimates.save_NWB(save_path, imaging_rate=fr, session_start_time=datetime.now(tzlocal()),\n",
    "                        raw_data_file=fnames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue to perform further analysis on this file by running the caiman.gui and loading the file. Y For example, if you are interested in df/F values, you can print them by executing the following line of code below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue to view specific estimates that you are further interested in analyzing and make plots (such as dF/F traces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm2.estimates.F_dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cnm2.estimates.F_dff[:,0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnm2.estimates.F_dff[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to use the dF/F data for further analysis in other script, you can store the data as a variable for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dFOverFv2 = cnm2.estimates.F_dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store dFOverFv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Using the Caiman Gui:\n",
    "\n",
    "You can load the NWB file created by running this script in the GUI. You can select the neurons and view their signals as well as the background of the video. You can alter any of the thresholds as you see fit. On the left bottom, you can change the spatial footprint of the neuron selected as well as the contrast of the background. On the right bottom, you can manually select the neurons that you want and add group as the accepted ones. You can also remove a single neuron from the accepted components which would move it to the rejected ones. You can save the changes to the NWB file or create an HDF5 file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
